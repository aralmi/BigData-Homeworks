from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, window, count, desc
from pyspark.sql.types import StructType, StructField, StringType, LongType

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark
spark = SparkSession.builder \
    .appName("TelegramStreamingAnalysis") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

print("=" * 80)
print("SPARK STREAMING CONSUMER –ò–ù–ò–¶–ò–ê–õ–ò–ó–ò–†–û–í–ê–ù")
print("=" * 80)

# –°—Ö–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Kafka
schema = StructType([
    StructField("username", StringType()),
    StructField("timestamp", StringType()),
    StructField("channel_id", LongType()),
    StructField("message_text", StringType())
])

try:
    # –ß–∏—Ç–∞–µ–º –∏–∑ Kafka
    df = spark.readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "localhost:9092") \
        .option("subscribe", "telegram_data") \
        .option("startingOffsets", "latest") \
        .load()

    # –ü–∞—Ä—Å–∏–º JSON
    parsed_df = df.select(
        from_json(col("value").cast("string"), schema).alias("data")
    ).select("data.*")

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp
    parsed_df = parsed_df.withColumn("timestamp_ts", col("timestamp").cast("timestamp"))

    # –ó–ê–î–ê–ù–ò–ï 1: –ü–æ–¥—Å—á—ë—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –∑–∞ 1 –º–∏–Ω—É—Ç—É
    window_1min = parsed_df \
        .groupBy(
            window(col("timestamp_ts"), "1 minute", "30 seconds"),
            "username"
        ) \
        .agg(count("*").alias("message_count")) \
        .select(
            col("window.start").alias("start"),
            col("window.end").alias("end"),
            "username",
            "message_count"
        )

    # –ó–ê–î–ê–ù–ò–ï 1: –ü–æ–¥—Å—á—ë—Ç —Å–æ–æ–±—â–µ–Ω–∏–π –∑–∞ 10 –º–∏–Ω—É—Ç
    window_10min = parsed_df \
        .groupBy(
            window(col("timestamp_ts"), "10 minute", "30 seconds"),
            "username"
        ) \
        .agg(count("*").alias("message_count")) \
        .select(
            col("window.start").alias("start"),
            col("window.end").alias("end"),
            "username",
            "message_count"
        )

    print("\n" + "=" * 80)
    print("üìä –ó–ê–î–ê–ù–ò–ï 1: –ü–æ–¥—Å—á—ë—Ç —Å–æ–æ–±—â–µ–Ω–∏–π (1 –∏ 10 –º–∏–Ω—É—Ç)")
    print("=" * 80 + "\n")

    # –í—ã–≤–æ–¥ –¥–ª—è 1 –º–∏–Ω—É—Ç—ã (–ë–ï–ó orderBy –≤ Update mode!)
    query_1min = window_1min \
        .writeStream \
        .outputMode("update") \
        .format("console") \
        .option("truncate", False) \
        .option("numRows", 50) \
        .start()

    # –í—ã–≤–æ–¥ –¥–ª—è 10 –º–∏–Ω—É—Ç (–ë–ï–ó orderBy –≤ Update mode!)
    query_10min = window_10min \
        .writeStream \
        .outputMode("update") \
        .format("console") \
        .option("truncate", False) \
        .option("numRows", 50) \
        .start()

    print("\nüéØ –ù–ê–ß–ê–õ–û –û–ë–†–ê–ë–û–¢–ö–ò –ü–û–¢–û–ö–ê –î–ê–ù–ù–´–•\n")

    # –ñ–¥–∏
    spark.streams.awaitAnyTermination()

except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    import traceback
    traceback.print_exc()
finally:
    spark.stop()


